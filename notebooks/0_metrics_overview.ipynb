{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python381064bitvenvvenvc5206c77ee25436a9af873d7e9575275",
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Metrics for this task"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Extractive summarization mainly measured by several word- and n-gram-similarity scoring algorithms, which calculate the fraction of target text that was produced by the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "From [StackExchange](https://datascience.stackexchange.com/a/75642/122618):\n",
    "\n",
    "*The problem with have an automatic evaluation system for a text summarisation model is that, although we can assess fluency from a language model, we can't really assess whether the model has pulled \"the most salient\" pieces of information from the original, longer text (and this can subjective from person to person). Hence why we need multiple human reference summaries to compute ROUGE and BLEU. However, as you are aware, these metrics have their limitations.*\n",
    "\n",
    "*ROUGE is essentially a further development of BLEU, which have been commonly used a dubious proxy for output text fluency in research to compare summarisation and translation models. These metrics are dubious because they simply look at how much they overlap with reference texts from humans (https://rxnlp.com/how-rouge-works-for-evaluation-of-summarization-tasks/#.Xt1ewy-ZOi4).*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Rouge\n",
    "Python implementation:\n",
    "https://github.com/IlyaGusev/rouge"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}